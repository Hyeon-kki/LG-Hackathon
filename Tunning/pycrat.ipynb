{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import re \n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import category_encoders as ce \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from sklearn.metrics import (    accuracy_score,    confusion_matrix,f1_score,precision_score, recall_score,roc_auc_score)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "class CFG:\n",
    "    user_seed = 42\n",
    "    target = 'is_converted'\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG.user_seed)\n",
    "\n",
    "\n",
    "import imblearn\n",
    "from pycaret.classification import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [feature for feature in train.columns if train[feature].dtype=='O']\n",
    "numeric = [f for f in train.columns if f not in categorical and f != 'is_converted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_corp = {\n",
    "    'Austria': ['LGEAG'],    'Czech Republic': ['LGECZ'],    'France': ['LGEFS'],    'Germany': ['LGEDG'],    'Greece': ['LGEHS'],    'Hungary': ['LGEMK'],    'Italy': ['LGEIS'],    'Netherlands': ['LGESC', 'LGEEH', 'LGEBN'],    'Poland': ['LGEWR', 'LGEPL', 'LGEMA'],    'Portugal': ['LGEPT','LGEBT'],\n",
    "    'EUs': ['LGEEB'],    'Romania': ['LGERO'],    'Spain': ['LGEES'],    'Sweden': ['LGENO', 'LGESW'],    'United Kingdom': ['LGEUK'],      'Kazakhstan': ['LGEAK'],    'Russia': ['LGERM', 'LGERI', 'LGERA'],\n",
    "    'Ukraine': ['LGEUR'],    'Latvia': ['LGELV','LGELA'],    'Algeria': ['LGEAS'],\n",
    "    'Egypt': ['LGEEG'],    'Jordan': ['LGELF'],    'Kenya': ['LGESK','LGEEF'],    'Morocco': ['LGEMC'],\n",
    "    'Saudi Arabia': ['LGESJ'],    'Iran':['LGEIR'],     'Israel':['LGEYK'],     'The Republic of South Africa': ['LGESA'],\n",
    "    'Tunisia': ['LGETU'],    'U.A.E': ['LGEOT', 'LGEDF', 'LGEGF', 'LGEME', 'LGEAF'],    'Nigeria': ['LGEAO', 'LGENI'],\n",
    "    'Turkey': ['LGETK', 'LGEAT'],    'Australia': ['LGEAP'],\n",
    "    'China': ['LGEQA', 'LGETL', 'LGECH', 'LGEYT', 'LGETR', 'LGETA', 'LGESY', 'LGESH', 'LGEQH', 'LGEQD', 'LGEPN', 'LGEND', 'LGEKS', 'LGEHZ', 'LGEHN', 'LGEHK'],\n",
    "    'India': ['LGEIL'],    'Indonesia': ['LGEIN'],    'Japan': ['LGEJP'],    'Malaysia': ['LGEML'],    'Philippines': ['LGEPH'],\n",
    "    'Singapore': ['LGESL'],    'Taiwan': ['LGETT'],    'Korea' :['LGEKR'],    'Thailand': ['LGETH'],    'Vietnam': ['LGEVN','LGEVH'],\n",
    "     'Canada': ['LGECI'],    'Mexico': ['LGERS', 'LGEMX', 'LGEMS', 'LGEMM'],    'United States': ['LGEMR', 'LGEUS', 'LGEMU', 'LGEAI'],\n",
    "    'Argentina': ['LGEAG','LGEAR'],    'Brazil': ['LGEBR','LGESP'],    'Chile': ['LGECL'],    'Colombia': ['LGEVZ', 'LGECB'],\n",
    "    'Panama': ['Guatemala', 'LGEPS'],    'Peru': ['LGEPR']}\n",
    "continent_nation={\n",
    "    'Europe':['EUs','Austria', 'Czech Republic' ,'France' ,'Germany', 'Greece' ,'Hungary', 'Italy', 'Netherlands' ,'Poland' ,'Portugal' ,'Romania', 'Spain' ,'Sweden','United Kingdom'], \n",
    "    'Russia and CIS':['Kazakhstan','Russia', 'Ukraine', 'Latvia'],     'Africa and MiddleEast': ['Israel','Iran','Algeria', 'Egypt', 'Jordan', 'Kenya', 'Morocco','Saudi Arabia','The Republic of South Africa','Tunisia', 'U.A.E', 'Nigeria', 'Turkey'], \n",
    "    'Asia':['Korea','Australia','China','India','Indonesia','Japan','Malaysia','Philippines','Singapore','Taiwan','Thailand','Vietnam'], \n",
    "    'NorthAmerica' : ['Canada','Mexico','United States'],    'SouthAmerica' :['Argentina','Brazil','Chile','Colombia','Panama','Peru']\n",
    "    \n",
    "}\n",
    "hemisphere = {\n",
    "    'Northern': ['EUs', 'Austria', 'Czech Republic', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Spain', 'Sweden', 'United Kingdom', 'Kazakhstan', 'Russia', 'Ukraine', 'Latvia', 'Israel', 'Iran', 'Jordan', 'Morocco', 'Saudi Arabia', 'Tunisia', 'Turkey', 'Korea', 'China', 'Japan', 'Taiwan', 'Canada', 'United States', 'Mexico', 'Panama'],\n",
    "    'Southern': ['Algeria', 'Egypt', 'Kenya', 'The Republic of South Africa', 'U.A.E', 'Nigeria', 'Australia', 'India', 'Indonesia', 'Malaysia', 'Philippines', 'Singapore', 'Thailand', 'Vietnam', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Peru']\n",
    "}\n",
    "mapping_dict = {\n",
    "#     \"Toi muon tim hieu thong tin ky thuat, gia ca cua sp de su dung\": \"Product Information\",\n",
    "#     \"tôi cần tham khảo giá và giải pháp từ LG\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Vui lòng báo giá giúp mình sản phẩm đo thân nhiệt Xin cảm ơn\": \"Request for quotation or purchase\",\n",
    "#     \"LED Signage\": \"Product Information\",\n",
    "#     \"Standalone\": \"Product Information\",\n",
    "#     \"for school\": \"Other\",\n",
    "#     \"Not specified\": \"Other\",\n",
    "#     \"Intégrateur historique du George V\": \"Other\",\n",
    "#     \"Solicito apoyo para realizar cotizacion de los dispositivos que ofrecen en la solución One Quick:\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Pantallas Interactivas para Clinicas\": \"Product Information\",\n",
    "#     \"Hotel TV products\": \"Product Information\",\n",
    "#     \"VRF\": \"Product Information\",\n",
    "#     \"Preciso de um monitor médico para radiografia convencional e tomogrtafia.\": \"Sales Inquiry\",\n",
    "    \"others\": \"Other\",\n",
    "    \"Others\": \"Other\",\n",
    "    \"other_\": \"Other\",\n",
    "    \"other\": \"Other\",\n",
    "    \"Etc.\": \"ETC.\",\n",
    "#     \"window facing product\": \"Product Information\",\n",
    "#     \"Digital platform\": \"Product Information\",\n",
    "#     \"(Select ID_Needs)\": \"Other\",\n",
    "#     \"One Quick:Flex\": \"Product Information\",\n",
    "#     \"AIO\": \"Product Information\",\n",
    "#     \"Needs\": \"Other\",\n",
    "#     \"Hospital TV\": \"Product Information\",\n",
    "#     \"i want to know the details about it\": \"Product Information\",\n",
    "#     \"EDUCATIONAL EQUIPMENTS\": \"Product Information\",\n",
    "#     \"TV interactive\": \"Product Information\",\n",
    "#     \"Hola me pueden cotizar 19 pantallas interactivas de 100 pulgadas entregadas en Guayaquil -Ecuador.\": \"Request for quotation or purchase\",\n",
    "#     \"teach\": \"Other\",\n",
    "#     \"Display Textbook and photos\": \"Usage or technical consultation\",\n",
    "#     \"High inch 86 / 98 or 110\": \"Product Information\",\n",
    "#     \"quotation_\": \"Request for quotation or purchase\",\n",
    "#     \"display product\": \"Product Information\",\n",
    "#     \"first Info and pricing\": \"Quotation or Purchase Consultation\",\n",
    "#     \"estoy buscando para Ecuador este producto LG MAGNIT micro LED, para un cliente de 138 pulgadas, con envió marítimo.\": \"Sales Inquiry\",\n",
    "#     \"Evento_SdelEstero\": \"Other\",\n",
    "#     \"probeam precio\": \"Sales Inquiry\",\n",
    "#     \"media inquiry\": \"Sales Inquiry\",\n",
    "#     \"Video Wall\": \"Product Information\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 생성 및 전처리 함수 \n",
    "def get_datas():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)\n",
    "    train['is_converted']=np.where(train['is_converted']==True,1,0)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def delete_cols(data, cols):\n",
    "    data = data.drop(columns=cols)\n",
    "    return data\n",
    "\n",
    "def log_transform(data,cols):\n",
    "    for col in cols :\n",
    "        data[col+'log']=np.log1p(data[col]) \n",
    "    return data \n",
    "\n",
    "\n",
    "def eda_expected_timeline(df):\n",
    "    \n",
    "    def timeline_label(time):\n",
    "    \n",
    "        time = str(time).lower().replace(' ','').replace('_','').replace('/','').replace(',','').replace('~','').replace('&','').replace('-','').replace('.','')\n",
    "        \n",
    "        if time == 'lessthan3months':\n",
    "            result = 'less than 3 months'\n",
    "        elif time == '3months6months':\n",
    "            result = '3 months ~ 6 months'\n",
    "        elif time == '6months9months':\n",
    "            result = '6 months ~ 9 months'\n",
    "        elif time == '9months1year':\n",
    "            result = '9 months ~ 1 year'\n",
    "        elif time == 'morethanayear':\n",
    "            result = 'more than a year'\n",
    "        else:\n",
    "            result = 'aimers_0203'\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    df['expected_timeline'] = df['expected_timeline'].apply(timeline_label)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# inquiry type 전처리하기 \n",
    "def eda_inquiry_type(df):\n",
    "    df['inquiry_type']= df['inquiry_type'].map(mapping_dict).fillna(train['inquiry_type'])\n",
    "    df.loc[df['inquiry_type'].str.contains('Solicito apoyo para realizar', na=False), 'inquiry_type'] = 'Quotation or Purchase Consultation'\n",
    "    df['inquiry_type'] = df['inquiry_type'].str.lower()\n",
    "    replacement = {'/': ' ', '-':' ', '_':' '}\n",
    "    df['inquiry_type'].replace(replacement, regex=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#customer type 처리 \n",
    "def customer_type(data):\n",
    "    data['customer_type']=data['customer_type'].fillna('none') \n",
    "    return data\n",
    "\n",
    "# total_area 변수로 통일\n",
    "def eda_business_area(df):\n",
    "    for col in ['business_area','business_subarea']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].str.replace(\" \", \"\") \n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', \"\") \n",
    "        df[col] = df[col].fillna('nan') \n",
    "    df['total_area'] = df['business_area'].astype(str) + df['business_subarea'].astype(str)\n",
    "    return df \n",
    "\n",
    "# 새로운 국가명, 대륙 열을 만들기 \n",
    "def get_nation_continent(df):\n",
    "    nation_corp_reverse ={v:k for k , values in nation_corp.items() for v in values }\n",
    "    df['nation']=df['response_corporate'].map(nation_corp_reverse)\n",
    "    continent_nation_reverse ={v:k for k , values in continent_nation.items() for v in values }\n",
    "    df['continent']=df['nation'].map(continent_nation_reverse)\n",
    "#     df = df.drop('customer_country',axis=1) \n",
    "    return df \n",
    "\n",
    "#라벨 인코딩 \n",
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "    return series\n",
    "\n",
    "# com_reg_ver_win_rate 최빈값으로 채우기 \n",
    "def com_reg_fill(train,test):\n",
    "    train['com_reg_ver_win_rate'] = train['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    test['com_reg_ver_win_rate'] = test['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    return train,test\n",
    "\n",
    "#****************************Feature Engineering*************************************#\n",
    "\n",
    "# area,unit,continent ->comregverwin , fe9 \n",
    "# area,unit ->ver_win_ratio_per_bu\n",
    "# unit, continent -> fe1 \n",
    "# owner ,unit  -> fe3 \n",
    "# nation, job -> fe10 \n",
    "\n",
    "# 회사별 Quotation or Purchase Consultation,Request for Partnership의 횟수\n",
    "# PortugalChina U.A.E  United States Argentina\n",
    "# 회사별 관료직, 혹은 성공률 높은 이들의 빈도 \n",
    "# unit, position 로 그룹화   x \n",
    "# 국가별 지점의 개수  \n",
    "# 국가별 성공률이 0.1이 넘는 국가 1 나머지 0 \n",
    "\n",
    "\n",
    "def fe_1(train,test):\n",
    "    # unit continent으로 엮어서 영업 전환율 살펴보기 -> 'unit_conti_mean'열 새로 생성\n",
    "    # 대륙별로 어느 사업부에 영업 성공율이 높은 지 \n",
    "    se=train.groupby(['business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    return train,test \n",
    "\n",
    "def fe_2(train,test):\n",
    "    # 영업 당담자가 어느 정도로 다양한 회사(customer_idx)을 담당하고 있는 지 \n",
    "    \n",
    "#     count = train.groupby('lead_owner').size().reset_index(name='leadowner_cnt')     \n",
    "#     train = train.merge(count, on='lead_owner', how='left')\n",
    "#     train['leadowner_cnt']= np.log1p(train['leadowner_cnt'])\n",
    "#     test = test.merge(count, on='lead_owner', how= 'left')\n",
    "#     test['leadowner_cnt']=np.log1p(test['leadowner_cnt'])\n",
    "    unique_count = train.groupby('lead_owner')['customer_idx'].nunique().reset_index(name='unique_cusidx_cnt')\n",
    "    train = train.merge(unique_count, on='lead_owner', how='left')\n",
    "    test = test.merge(unique_count,on ='lead_owner',how ='left')\n",
    "    train['unique_cusidx_cnt']= np.log1p(train['unique_cusidx_cnt'])\n",
    "    test['unique_cusidx_cnt']= np.log1p(test['unique_cusidx_cnt'])\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "def fe_3(train,test):\n",
    "    # 영업담당자와 사업부로 영업전환 성공률 살펴보기 -> 어느 사업부를 어느 담당자가 담당해야 성공율이 높나 확인 \n",
    "\n",
    "    se = train.groupby(['lead_owner','business_unit'])['is_converted'].agg(['mean']).rename(columns={'mean': 'owner_unit_mean'})\n",
    "    train = train.merge(se, on=['lead_owner','business_unit'], how='left')\n",
    "    test = test.merge(se, on=['lead_owner','business_unit'],how='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_4(train,test):\n",
    "    # customer_idx가 대기업, 중소기업으로 분류되는 경우 1을 부여 \n",
    "    se = train[train.groupby('customer_idx')['enterprise'].transform('nunique') > 1]\n",
    "    multi_company=list(se['customer_idx'].unique())\n",
    "    train['multi_company']=np.where(train['customer_idx'].isin(multi_company) ,1,0)\n",
    "    test['multi_company']=np.where(test['customer_idx'].isin(multi_company) ,1,0)\n",
    "    return train, test\n",
    "\n",
    "def fe_5(train,test):\n",
    "    # LG지점 , 사업부 , bantsubmit으로 영업 성공율 살펴보기 -> 너무 과적합됨으로 제외 \n",
    "    se = train.groupby(['response_corporate','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    test=test.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "def fe_6(train,test):\n",
    "    # 영업사원, 사업부, bandsubmit 으로 영업 성공율 살펴보기 -> 과적합으로 제외 \n",
    "    se = train.groupby(['lead_owner','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    test =test.merge(se,on =['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_7(train,test):\n",
    "    # bant submit 제곱하기 -> isconverted와 corr는 더 높지만 성능향상은 없음 \n",
    "    train['bant_submit']=train['bant_submit']*train['bant_submit']\n",
    "    test['bant_submit']=test['bant_submit']*test['bant_submit']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_8(df):\n",
    "    # 국가별로 북반구와 남반구 특성을 생성하기 \n",
    "    hemisphere_reverse ={v:k for k , values in hemisphere.items() for v in values }\n",
    "    df['hemisphere'] =df['nation'].map(hemisphere_reverse)\n",
    "    return df \n",
    "\n",
    "def fe_9(train,test):\n",
    "    # 대륙별,사업 분야별, 사업부로 영업 성공률 살피기 \n",
    "    se=train.groupby(['business_area','business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'area_unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_10(train,test):\n",
    "    #국가별, 고객의 직업에 따라서 영업 성공율 살펴보기 \n",
    "    se =train.groupby(['nation','customer_job'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_job_mean'})\n",
    "    \n",
    "    train =train.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    train['nat_job_mean']=train['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    test['nat_job_mean']=test['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    return train,test \n",
    "\n",
    "\n",
    "\n",
    "# def fe_11(train,test):\n",
    "#     se1 =train.groupby(['business_unit','business_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'new_perbu'})\n",
    "#     train =train.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     test =test.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     train = train.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "    \n",
    "#     test = test.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "#     return train,test \n",
    "\n",
    "def fe_12(train,test):\n",
    "    train['com_product'] = train['product_category'].apply(lambda x: 1 if 'signage' in str(x) else 0)\n",
    "    \n",
    "    se= train.groupby(['customer_idx'])['com_product'].agg(['mean']).rename(columns={'mean':'com_prod_mean'})\n",
    "    train = train.merge(se, on =['customer_idx'], how ='left')\n",
    "    test = test.merge(se, on =['customer_idx'], how ='left')\n",
    "    train= train.drop('com_product', axis= 1 )\n",
    "    return train,test\n",
    "    \n",
    "def fe_13(train,test):\n",
    "    se = train.groupby(['nation', 'inquiry_type'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_inquiry_type_mean'})\n",
    "    train =train.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    train['nat_inquiry_type_mean']=train['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    test['nat_inquiry_type_mean']=test['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_14(train,test):\n",
    "    se =train.groupby(['nation'])['response_corporate'].agg(['count']).rename(columns={'count':'nr_count'})\n",
    "    train = train.merge(se, on='nation', how='left')\n",
    "    test  = test.merge(se,on='nation',how='left')\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def fe_15(train,test):\n",
    "    se =train.groupby(['business_unit','continent','customer_position'])['is_converted'].agg(['mean']).rename(columns={'mean':'unit_conti_pos'})\n",
    "    \n",
    "    train =train.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    train['unit_conti_pos']=train['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    test['unit_conti_pos']=test['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_16(train,test):\n",
    "    se =train.groupby(['continent','total_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'total_conti_pos'})\n",
    "    train =train.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    train['total_conti_pos']=train['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    test['total_conti_pos']=test['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def fe_17(train,test):\n",
    "    train['good']=0 \n",
    "    test['good'] = 0 \n",
    "    train.loc[train['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    test.loc[test['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    return train,test \n",
    "\n",
    "def fe_18(train,test, col1,col2):\n",
    "    time_avg = train[[col2, 'is_converted']].groupby(col2).mean()\n",
    "    time_avg.columns = [f'{col2}_avg']\n",
    "\n",
    "    timeline = train.loc[train[col2] != 'weoif', col1 + [col2]]\n",
    "    timeline['cnt'] = 1\n",
    "    timeline_se = timeline.groupby(col1 + [col2]).count()\n",
    "    timeline_se.reset_index(inplace =True)\n",
    "    temp2 = pd.merge(timeline_se, time_avg, how = 'left' , on=[col2])\n",
    "    temp2['multip'] = temp2['cnt'] * temp2[f'{col2}_avg']\n",
    "    temp2 = temp2.groupby(col1).sum().reset_index().drop([f'{col2}_avg'], axis =1)\n",
    "\n",
    "    temp2[f'{col2}_mean'] = temp2['multip'] / temp2['cnt']\n",
    "    temp2.drop(['multip','cnt'], axis=1 , inplace= True)\n",
    "\n",
    "    train= pd.merge(train, temp2, how ='left' , on=col1)\n",
    "    test= pd.merge(test, temp2, how ='left' , on=col1)\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def create_grouped_features(train, test, group, numeric_var):\n",
    "    # 범주형 특성들에 대해서 다른 수치형 데이터의 중앙값, 최대, 합을 새로운 열로 추가하기 \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    aggs = ['median', 'max','sum']\n",
    "    for agg in aggs:\n",
    "        # groupby 후 aggregation\n",
    "        a1 = train.groupby([group])[numeric_var].agg(agg).to_dict()\n",
    "        # 새로운 feature 생성\n",
    "        train[numeric_var+'_'+group+'_'+agg] = train[group].map(a1)\n",
    "        test[numeric_var+'_'+group+'_'+agg] = test[group].map(a1)\n",
    "    return train, test\n",
    "\n",
    "def do_scale(train,test, scale_cols) :\n",
    "    for c in scale_cols:\n",
    "        min_value = train[c].min()\n",
    "        max_value = train[c].max()\n",
    "        train[c+'sc'] = (train[c] - min_value) / (max_value - min_value)\n",
    "        test[c+'sc'] = (test[c] - min_value) / (max_value - min_value)\n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['business_unit','customer_idx']\n",
    "numeric_vars = ['historical_existing_cnt', 'lead_desc_length']\n",
    "scale_cols = ['com_reg_ver_win_rate','historical_existing_cnt', 'lead_desc_length','ver_win_rate_x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "\n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type(train) ,customer_type(test)\n",
    "train,test=eda_inquiry_type(train) ,eda_inquiry_type(test)\n",
    "\n",
    "# Feature Engineering \n",
    "train,test = fe_1(train,test)\n",
    "train,test = fe_2(train,test)\n",
    "train,test = fe_3(train,test)\n",
    "train,test = fe_9(train,test)\n",
    "train,test = fe_18(train,test, ['continent', 'bant_submit'],'inquiry_type')\n",
    "\n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CatBoostEncoder(a=1,\n",
       "                cols=[&#x27;expected_timeline&#x27;, &#x27;total_area&#x27;, &#x27;customer_country.1&#x27;,\n",
       "                      &#x27;product_category&#x27;, &#x27;customer_position&#x27;,\n",
       "                      &#x27;business_subarea&#x27;, &#x27;customer_country&#x27;,\n",
       "                      &#x27;product_modelname&#x27;, &#x27;response_corporate&#x27;, &#x27;continent&#x27;,\n",
       "                      &#x27;product_subcategory&#x27;, &#x27;business_unit&#x27;, &#x27;customer_job&#x27;,\n",
       "                      &#x27;customer_type&#x27;, &#x27;business_area&#x27;, &#x27;enterprise&#x27;],\n",
       "                drop_invariant=False, handle_missing=&#x27;value&#x27;,\n",
       "                handle_unknown=&#x27;value&#x27;, random_state=None, return_df=True,\n",
       "                sigma=None, verbose=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostEncoder</label><div class=\"sk-toggleable__content\"><pre>CatBoostEncoder(a=1,\n",
       "                cols=[&#x27;expected_timeline&#x27;, &#x27;total_area&#x27;, &#x27;customer_country.1&#x27;,\n",
       "                      &#x27;product_category&#x27;, &#x27;customer_position&#x27;,\n",
       "                      &#x27;business_subarea&#x27;, &#x27;customer_country&#x27;,\n",
       "                      &#x27;product_modelname&#x27;, &#x27;response_corporate&#x27;, &#x27;continent&#x27;,\n",
       "                      &#x27;product_subcategory&#x27;, &#x27;business_unit&#x27;, &#x27;customer_job&#x27;,\n",
       "                      &#x27;customer_type&#x27;, &#x27;business_area&#x27;, &#x27;enterprise&#x27;],\n",
       "                drop_invariant=False, handle_missing=&#x27;value&#x27;,\n",
       "                handle_unknown=&#x27;value&#x27;, random_state=None, return_df=True,\n",
       "                sigma=None, verbose=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CatBoostEncoder(a=1,\n",
       "                cols=['expected_timeline', 'total_area', 'customer_country.1',\n",
       "                      'product_category', 'customer_position',\n",
       "                      'business_subarea', 'customer_country',\n",
       "                      'product_modelname', 'response_corporate', 'continent',\n",
       "                      'product_subcategory', 'business_unit', 'customer_job',\n",
       "                      'customer_type', 'business_area', 'enterprise'],\n",
       "                drop_invariant=False, handle_missing='value',\n",
       "                handle_unknown='value', random_state=None, return_df=True,\n",
       "                sigma=None, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_delete=['nation']\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\",  \n",
    "     \"expected_timeline\",\n",
    "'nation','continent',\n",
    "'total_area']\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('int64'), dtype('O'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('int64'), dtype('int64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('int64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('int64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('int64'), dtype('int64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['bant_submit', 'customer_country', 'business_unit',\n",
    "       'com_reg_ver_win_rate', 'customer_idx', 'customer_type',\n",
    "       'enterprise', 'historical_existing_cnt', 'id_strategic_ver',\n",
    "       'it_strategic_ver', 'idit_strategic_ver', 'customer_job',\n",
    "       'lead_desc_length', 'inquiry_type', 'product_category',\n",
    "       'product_subcategory', 'product_modelname', 'customer_country.1',\n",
    "       'customer_position', 'response_corporate', 'expected_timeline',\n",
    "       'ver_cus', 'ver_pro', 'ver_win_rate_x', 'ver_win_ratio_per_bu',\n",
    "       'business_area', 'business_subarea', 'lead_owner',\n",
    "       'com_reg_ver_win_ratesc', 'historical_existing_cntsc',\n",
    "       'lead_desc_lengthsc', 'ver_win_rate_xsc',\n",
    "       'historical_existing_cnt_business_unit_median',\n",
    "       'historical_existing_cnt_business_unit_max',\n",
    "       'historical_existing_cnt_business_unit_sum',\n",
    "       'lead_desc_length_business_unit_median',\n",
    "       'lead_desc_length_business_unit_max',\n",
    "       'lead_desc_length_business_unit_sum',\n",
    "       'historical_existing_cnt_customer_idx_median',\n",
    "       'historical_existing_cnt_customer_idx_max',\n",
    "       'historical_existing_cnt_customer_idx_sum',\n",
    "       'lead_desc_length_customer_idx_median',\n",
    "       'lead_desc_length_customer_idx_max',\n",
    "       'lead_desc_length_customer_idx_sum', 'com_reg_ver_win_ratelog',\n",
    "       'lead_desc_lengthlog', 'total_area', 'continent',\n",
    "       'unit_conti_mean', 'unique_cusidx_cnt', 'owner_unit_mean',\n",
    "       'area_unit_conti_mean', 'inquiry_type_mean', 'customer_idxcount',\n",
    "       'customer_typecount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'sales inquiry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_model \u001b[38;5;241m=\u001b[39m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumeric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstratifiedkfold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfix_imbalance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# data imbalance 를 sampling method로 보정\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimblearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover_sampling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomOverSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/classification/functional.py:595\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(data, data_func, target, index, train_size, test_data, ordinal_features, numeric_features, categorical_features, date_features, text_features, ignore_features, keep_features, preprocess, create_date_columns, imputation_type, numeric_imputation, categorical_imputation, iterative_imputation_iters, numeric_iterative_imputer, categorical_iterative_imputer, text_features_method, max_encoding_ohe, encoding_method, rare_to_value, rare_value, polynomial_features, polynomial_degree, low_variance_threshold, group_features, drop_groups, remove_multicollinearity, multicollinearity_threshold, bin_numeric_features, remove_outliers, outliers_method, outliers_threshold, fix_imbalance, fix_imbalance_method, transformation, transformation_method, normalize, normalize_method, pca, pca_method, pca_components, feature_selection, feature_selection_method, feature_selection_estimator, n_features_to_select, custom_pipeline, custom_pipeline_position, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, html, session_id, system_log, log_experiment, experiment_name, experiment_custom_tags, log_plots, log_profile, log_data, verbose, memory, profile, profile_kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m exp \u001b[38;5;241m=\u001b[39m _EXPERIMENT_CLASS()\n\u001b[1;32m    594\u001b[0m set_current_experiment(exp)\n\u001b[0;32m--> 595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordinal_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordinal_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_date_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_date_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimputation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimputation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_features_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoding_ohe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoding_ohe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrare_to_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_to_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrare_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_variance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_variance_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_numeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_numeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_outliers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutliers_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutliers_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutliers_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutliers_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_stratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_stratify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_experiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_experiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_plots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_profile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_profile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/classification/oop.py:890\u001b[0m, in \u001b[0;36mClassificationExperiment.setup\u001b[0;34m(self, data, data_func, target, index, train_size, test_data, ordinal_features, numeric_features, categorical_features, date_features, text_features, ignore_features, keep_features, preprocess, create_date_columns, imputation_type, numeric_imputation, categorical_imputation, iterative_imputation_iters, numeric_iterative_imputer, categorical_iterative_imputer, text_features_method, max_encoding_ohe, encoding_method, rare_to_value, rare_value, polynomial_features, polynomial_degree, low_variance_threshold, group_features, drop_groups, remove_multicollinearity, multicollinearity_threshold, bin_numeric_features, remove_outliers, outliers_method, outliers_threshold, fix_imbalance, fix_imbalance_method, transformation, transformation_method, normalize, normalize_method, pca, pca_method, pca_components, feature_selection, feature_selection_method, feature_selection_estimator, n_features_to_select, custom_pipeline, custom_pipeline_position, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, html, session_id, system_log, log_experiment, experiment_name, experiment_custom_tags, log_plots, log_profile, log_data, engine, verbose, memory, profile, profile_kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39msteps\u001b[38;5;241m.\u001b[39mremove((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished creating preprocessing pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/pipeline.py:270\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    269\u001b[0m     fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 270\u001b[0m     X, y, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/pipeline.py:246\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    243\u001b[0m         cloned\u001b[38;5;241m.\u001b[39m_cache_full_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Fit or load the current transformer from cache\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_memory_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcloned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_transform(\n\u001b[1;32m    254\u001b[0m         transformer\u001b[38;5;241m=\u001b[39mfitted_transformer,\n\u001b[1;32m    255\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    256\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# transformer (necessary when loading from the cache)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/joblib/memory.py:655\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/memory.py:398\u001b[0m, in \u001b[0;36mFastMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# PYCARET CHANGES\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_output_identifiers \u001b[38;5;241m=\u001b[39m func_id, args_id\n\u001b[0;32m--> 398\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;66;03m# PYCARET CHANGES END\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/memory.py:309\u001b[0m, in \u001b[0;36mFastMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# PYCARET CHANGES\u001b[39;00m\n\u001b[1;32m    308\u001b[0m func_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 309\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m func_duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m func_start_time\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_duration \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_time_to_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/pipeline.py:68\u001b[0m, in \u001b[0;36m_fit_one\u001b[0;34m(transformer, X, y, message, **fit_params)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m signature(transformer\u001b[38;5;241m.\u001b[39mfit)\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m     67\u001b[0m             args\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m---> 68\u001b[0m         \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformer\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/pycaret/internal/preprocess/transformers.py:229\u001b[0m, in \u001b[0;36mTransformerWrapper.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m transformer_params \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/impute/_base.py:390\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    382\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[0;32m--> 390\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/impute/_base.py:342\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[1;32m    337\u001b[0m     new_ve \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, ve\n\u001b[1;32m    340\u001b[0m         )\n\u001b[1;32m    341\u001b[0m     )\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: 'sales inquiry'"
     ]
    }
   ],
   "source": [
    "\n",
    "my_model = setup(session_id=CFG.user_seed, \n",
    "                 data=train, \n",
    "                 target=CFG.target, \n",
    "                 normalize=False, normalize_method='zscore',\n",
    "                 transformation=False,  \n",
    "                 numeric_features = numeric,\n",
    "                 fold_strategy='stratifiedkfold',\n",
    "                 fold=10,\n",
    "                 fold_shuffle=True,\n",
    "                 fix_imbalance = True, # data imbalance 를 sampling method로 보정\n",
    "                 fix_imbalance_method = imblearn.over_sampling.RandomOverSampler(),\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
